{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***The Cross-Entropy Method***\n",
    "\n",
    "The Cross-Entropy Method is model-free and policy-based. In RL, methods are:\n",
    "\n",
    "- Model-free or model-based\n",
    "- Value-based or policy-based\n",
    "- On-policy or off-policy\n",
    "\n",
    "The term \"model-free\" means that the method doesn't build a model of the environment or reward; it just directly connects observations to actions (or values that are related to actions). In contrast, model-based methods try to predict what the next observation and/or reward will be.\n",
    "\n",
    "Policy-based methods directly approximate the policy of the agent, that is, what actions the agent should carry out at every step. The policy is usually represented by a probability distribution over the available actions. In contrast, the method could be value-based. In this case, instead of the probability of actions, the agent calculates the value of every possible action and chooses the action with the best value.\n",
    "\n",
    "For now, define off-policy as the ability of the method to learn on historical data (obtained by a previous version of the agent, recorded by human demonstration, or just seen by the same agent several episodes ago).\n",
    "\n",
    "In practice, the policy is usually represented as a probability distribution over actions, which makes it very similar to a classification problem, with the amount of classes being equal to the amount of actions we can carry out. \n",
    "\n",
    "### ***The Method in Action***\n",
    "\n",
    "The core of the cross-entropy method is to throw away bad episodes and train on better ones. So, the steps of the method are as follows:\n",
    "1. Play N number of episodes using our current model and environment.\n",
    "2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
    "3. Throw away all episodes with a reward below the boundary.\n",
    "4. Train on the remaining \"elite\" episodes using observations as the input and issued actions as the desired output.\n",
    "5. Repeat from step 1 until we become satisfied with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from dataclasses import dataclass\n",
    "import typing as tt\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpisodeStep:\n",
    "    \"\"\"Represents one single step that our agent makes in an episode.\"\"\"\n",
    "    observation: np.ndarray\n",
    "    action: int\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    \"\"\"Single episode stored as total undiscounted reward and a collection of EpisodeStep.\"\"\"\n",
    "    reward: float\n",
    "    steps: tt.List[EpisodeStep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env: gym.Env, net: Net, batch_size: int) -> tt.Generator[tt.List[Episode], None, None]:\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs, _ = env.reset()\n",
    "    sm = nn.Softmax(dim = 1) # Used to convert NN output to a probability distribution\n",
    "\n",
    "    while True:\n",
    "        obs_v = torch.tensor(obs, dtype = torch.float32)\n",
    "        act_porbs_v = sm(net(obs_v.unsqueeze(0)))\n",
    "        act_probs = act_porbs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p = act_probs)\n",
    "        next_obs, reward, is_done, is_trunc, _ = env.step(action)\n",
    "        episode_reward += float(reward)\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done or is_trunc:\n",
    "            e = Episode(reward = episode_reward, steps = episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs, _ = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch: tt.List[Episode], percentile: float) -> \\\n",
    "        tt.Tuple[torch.FloatTensor, torch.LongTensor, float, float]:\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = float(np.percentile(rewards, percentile))\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs: tt.List[np.ndarray] = []\n",
    "    train_act: tt.List[int] = []\n",
    "    for episode in batch:\n",
    "        if episode.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, episode.steps))\n",
    "        train_act.extend(map(lambda step: step.action, episode.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(np.vstack(train_obs))\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "0: loss=0.701, reward_mean=19.0, rw_bound=21.0\n",
      "1: loss=0.663, reward_mean=13.9, rw_bound=16.0\n",
      "2: loss=0.606, reward_mean=15.7, rw_bound=17.5\n",
      "3: loss=0.575, reward_mean=14.5, rw_bound=17.0\n",
      "4: loss=0.558, reward_mean=16.1, rw_bound=17.5\n",
      "5: loss=0.629, reward_mean=18.1, rw_bound=18.0\n",
      "6: loss=0.584, reward_mean=20.4, rw_bound=20.5\n",
      "7: loss=0.669, reward_mean=18.8, rw_bound=22.0\n",
      "8: loss=0.644, reward_mean=20.9, rw_bound=24.5\n",
      "9: loss=0.676, reward_mean=31.6, rw_bound=36.5\n",
      "10: loss=0.648, reward_mean=36.9, rw_bound=43.0\n",
      "11: loss=0.645, reward_mean=32.0, rw_bound=33.5\n",
      "12: loss=0.627, reward_mean=37.8, rw_bound=40.0\n",
      "13: loss=0.623, reward_mean=33.2, rw_bound=40.0\n",
      "14: loss=0.615, reward_mean=42.9, rw_bound=42.5\n",
      "15: loss=0.609, reward_mean=43.3, rw_bound=51.0\n",
      "16: loss=0.591, reward_mean=45.8, rw_bound=53.0\n",
      "17: loss=0.594, reward_mean=58.1, rw_bound=65.5\n",
      "18: loss=0.590, reward_mean=60.4, rw_bound=76.5\n",
      "19: loss=0.595, reward_mean=75.2, rw_bound=72.0\n",
      "20: loss=0.568, reward_mean=68.0, rw_bound=86.0\n",
      "21: loss=0.591, reward_mean=64.6, rw_bound=76.0\n",
      "22: loss=0.563, reward_mean=67.8, rw_bound=87.0\n",
      "23: loss=0.583, reward_mean=53.1, rw_bound=62.0\n",
      "24: loss=0.553, reward_mean=57.9, rw_bound=66.0\n",
      "25: loss=0.573, reward_mean=79.9, rw_bound=95.5\n",
      "26: loss=0.565, reward_mean=73.0, rw_bound=82.0\n",
      "27: loss=0.579, reward_mean=68.7, rw_bound=80.5\n",
      "28: loss=0.546, reward_mean=93.2, rw_bound=102.5\n",
      "29: loss=0.551, reward_mean=97.3, rw_bound=125.5\n",
      "30: loss=0.560, reward_mean=102.4, rw_bound=131.5\n",
      "31: loss=0.556, reward_mean=117.2, rw_bound=132.5\n",
      "32: loss=0.544, reward_mean=138.3, rw_bound=167.0\n",
      "33: loss=0.533, reward_mean=119.3, rw_bound=122.5\n",
      "34: loss=0.547, reward_mean=133.6, rw_bound=140.0\n",
      "35: loss=0.525, reward_mean=125.9, rw_bound=134.0\n",
      "36: loss=0.547, reward_mean=123.0, rw_bound=133.0\n",
      "37: loss=0.535, reward_mean=153.4, rw_bound=142.5\n",
      "38: loss=0.561, reward_mean=130.4, rw_bound=157.5\n",
      "39: loss=0.539, reward_mean=153.6, rw_bound=185.0\n",
      "40: loss=0.522, reward_mean=152.0, rw_bound=166.0\n",
      "41: loss=0.528, reward_mean=158.8, rw_bound=177.5\n",
      "42: loss=0.523, reward_mean=171.9, rw_bound=195.0\n",
      "43: loss=0.504, reward_mean=199.6, rw_bound=213.5\n",
      "44: loss=0.529, reward_mean=175.7, rw_bound=177.0\n",
      "45: loss=0.509, reward_mean=157.8, rw_bound=171.0\n",
      "46: loss=0.518, reward_mean=186.9, rw_bound=211.5\n",
      "47: loss=0.507, reward_mean=201.8, rw_bound=219.5\n",
      "48: loss=0.520, reward_mean=181.4, rw_bound=230.0\n",
      "49: loss=0.510, reward_mean=245.9, rw_bound=247.0\n",
      "50: loss=0.503, reward_mean=287.2, rw_bound=324.5\n",
      "51: loss=0.518, reward_mean=288.3, rw_bound=349.5\n",
      "52: loss=0.506, reward_mean=327.8, rw_bound=360.0\n",
      "53: loss=0.520, reward_mean=336.9, rw_bound=423.0\n",
      "54: loss=0.527, reward_mean=381.5, rw_bound=420.0\n",
      "55: loss=0.522, reward_mean=396.1, rw_bound=500.0\n",
      "56: loss=0.526, reward_mean=421.1, rw_bound=500.0\n",
      "57: loss=0.525, reward_mean=461.8, rw_bound=500.0\n",
      "58: loss=0.527, reward_mean=465.8, rw_bound=500.0\n",
      "59: loss=0.525, reward_mean=455.1, rw_bound=500.0\n",
      "60: loss=0.525, reward_mean=453.2, rw_bound=500.0\n",
      "61: loss=0.520, reward_mean=497.4, rw_bound=500.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    assert env.observation_space.shape is not None\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "    n_actions = int(env.action_space.n)\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    print(net)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 475:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVIEW MATERIAL: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
