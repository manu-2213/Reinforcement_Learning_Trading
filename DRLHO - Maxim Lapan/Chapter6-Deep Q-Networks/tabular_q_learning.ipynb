{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-learning\n",
    "\n",
    "We have an environment that can be used as a source of real-life samples of states. We can use states obtained from the environment to update the values of states, which can save us a lot of work.\n",
    "\n",
    "This modification of the value iteration method is known as **Q-learning**, and for cases with explicit state-to-value mappings, it has the following steps:\n",
    "\n",
    "1. **Initialization:**  \n",
    "   Start with an empty table, mapping states to values of actions.\n",
    "\n",
    "2. **Sampling:**  \n",
    "   By interacting with the environment, obtain the tuple $(s, a, r, s')$, where:\n",
    "   - $s$ is the current state,\n",
    "   - $a$ is the action taken,\n",
    "   - $r$ is the reward received,\n",
    "   - $s'$ is the next state.\n",
    "\n",
    "   In this step, you need to decide which action to take, and there is no single proper way to make this decision. This decision-making process relates to the exploration versus exploitation trade-off.\n",
    "\n",
    "3. **Bellman Update:**  \n",
    "   Update the $Q(s, a)$ value using the Bellman approximation:\n",
    "   $$\n",
    "   Q(s, a) \\leftarrow r + \\gamma \\max_{a' \\in A} Q(s', a')\n",
    "   $$\n",
    "\n",
    "4. **Iteration:**  \n",
    "   Repeat from step 2 until the convergence conditions are met (e.g., when the updates fall below a predefined threshold or when the expected reward from the policy is satisfactorily estimated).\n",
    "\n",
    "> **Note:** As we take samples from the environment, it's generally a bad idea to just assign new values on top of existing values, as training can become unstable.\n",
    "\n",
    "# Deep Q-Networks\n",
    "\n",
    "In practice, updating the $Q(s, a)$ values is often done using a \"blending\" technique, which involves averaging between old and new values. This is achieved by using a learning rate $\\alpha$ (with $\\alpha \\in [0,1]$), allowing the updates to be smooth and stable, even in a noisy environment.\n",
    "\n",
    "The update rule is:\n",
    "$$\n",
    "Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a' \\in A} Q(s', a') \\right)\n",
    "$$\n",
    "\n",
    "The final version of the algorithm is as follows:\n",
    "\n",
    "1. **Initialization:**  \n",
    "   Start with an empty table for $Q(s, a)$.\n",
    "\n",
    "2. **Sampling:**  \n",
    "   Obtain $(s, a, r, s')$ from the environment.\n",
    "\n",
    "3. **Bellman Update:**  \n",
    "   Perform the update using:\n",
    "   $$\n",
    "   Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a' \\in A} Q(s', a') \\right)\n",
    "   $$\n",
    "\n",
    "4. **Convergence Check:**  \n",
    "   Check if the convergence conditions are met. If not, repeat from step 2.\n",
    "\n",
    "This method is called **tabular Q-learning**, as we maintain a table of states with their corresponding $Q$-values. Let's try applying this approach on our FrozenLake environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "State = int\n",
    "Action = int\n",
    "ValuesKey = tt.Tuple[State, Action]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.values: tt.Dict[ValuesKey] = defaultdict(float)\n",
    "\n",
    "    def sample_env(self) -> tt.Tuple[State, Action, float, State]:\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
    "        if is_done or is_tr:\n",
    "            self.state, _ = self.env.reset()\n",
    "        else:\n",
    "            self.state = new_state\n",
    "        return old_state, action, float(reward), new_state\n",
    "    \n",
    "    def best_value_and_action(self, state: State) -> tt.Tuple[float, Action]:\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "    \n",
    "    def value_update(self, state: State, action: Action, reward: float, next_state: State):\n",
    "        best_val, _ = self.best_value_and_action(next_state)\n",
    "        new_val = reward + GAMMA * best_val\n",
    "        old_val = self.values[(state, action)]\n",
    "        key = (state, action)\n",
    "        self.values[key] = old_val * (1 - ALPHA) + new_val * ALPHA\n",
    "\n",
    "    def play_episiode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, is_tr, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done or is_tr:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884: Best test reward updated 0.000 -> 0.250\n",
      "885: Best test reward updated 0.250 -> 0.350\n",
      "941: Best test reward updated 0.350 -> 0.400\n",
      "1219: Best test reward updated 0.400 -> 0.450\n",
      "1744: Best test reward updated 0.450 -> 0.550\n",
      "4547: Best test reward updated 0.550 -> 0.650\n",
      "5770: Best test reward updated 0.650 -> 0.700\n",
      "6790: Best test reward updated 0.700 -> 0.800\n",
      "6801: Best test reward updated 0.800 -> 0.850\n",
      "6902: Best test reward updated 0.850 -> 0.900\n",
      "6911: Best test reward updated 0.900 -> 0.950\n",
      "Solved in 6911 iterations\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        state, action, reward, next_state = agent.sample_env()\n",
    "        agent.value_update(state, action, reward, next_state)\n",
    "\n",
    "        test_reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            test_reward += agent.play_episiode(test_env)\n",
    "        test_reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", test_reward, iter_no)\n",
    "        if test_reward > best_reward:\n",
    "            print(\"%d: Best test reward updated %.3f -> %.3f\" % (iter_no, best_reward, test_reward))\n",
    "            best_reward = test_reward\n",
    "        if test_reward > 0.90:\n",
    "            print(\"Solved in %d iterations\" %iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
