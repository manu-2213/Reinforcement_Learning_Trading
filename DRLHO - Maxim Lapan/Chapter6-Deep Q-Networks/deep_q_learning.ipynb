{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In deep Q-learning, we replace the table of Q-values with a neural network approximation, denoted as $Q(s,a; \\theta)$, where $\\theta$ represents the network parameters. The modifications to the standard Q-learning algorithm are as follows:\n",
    "\n",
    "1. **Initialization:**  \n",
    "   Initialize the Q-network $Q(s, a; \\theta)$ with some initial approximation (typically random weights).\n",
    "\n",
    "2. **Experience Sampling:**  \n",
    "   Interact with the environment to obtain the tuple $(s, a, r, s')$. In practice, experiences are often stored in a replay buffer to decorrelate samples.\n",
    "\n",
    "3. **Loss Calculation:**  \n",
    "   Compute the loss $\\mathcal{L}$:\n",
    "   - If the episode has ended:\n",
    "     $$\n",
    "     \\mathcal{L} = \\left( Q(s, a; \\theta) - r \\right)^2\n",
    "     $$\n",
    "   - Otherwise:\n",
    "     $$\n",
    "     \\mathcal{L} = \\left( Q(s, a; \\theta) - \\left( r + \\gamma \\max_{a' \\in A} Q(s', a'; \\theta) \\right) \\right)^2\n",
    "     $$\n",
    "\n",
    "4. **Parameter Update:**  \n",
    "   Update the network parameters $\\theta$ using stochastic gradient descent (SGD) to minimize the loss $\\mathcal{L}$.\n",
    "\n",
    "5. **Iteration:**  \n",
    "   Repeat from step 2 until convergence.\n",
    "\n",
    "*Note:* In practical implementations, additional techniques such as target networks and experience replay are used to improve stability and performance.\n",
    "\n",
    "Random behavior is better at the beginning of the training when our Q approximation is bad, as it gives us more uniformly distributed information about the environment states. As our training progresses, random behavior becomes inefficient, and we want to fall back to our Q approximation to decide how to act.\n",
    "\n",
    "# SGD Optimization in Deep Q-Learning\n",
    "\n",
    "Deep Q-learning treats Q-value approximation as a supervised learning problem, using **Stochastic Gradient Descent (SGD)**. However, RL data violates the **i.i.d. assumption** (one of the fundamental requirements for SGD optimization is that the training data is independent and identically distributed) because:\n",
    "\n",
    "1. **Non-Independent Samples:** Consecutive experiences are highly correlated as they belong to the same episode.\n",
    "2. **Non-Identical Distribution:** Training data comes from a suboptimal policy (e.g., random or $\\epsilon$-greedy), while the goal is to learn an optimal policy.\n",
    "\n",
    "To address this, **Replay Buffers** store past experiences and sample from them to create more independent training batches. This stabilizes training and ensures the model learns from recent yet diverse experiences.\n",
    "\n",
    "### **Correlation Between Steps:**  \n",
    "Bootstrapping with the Bellman equation links $Q(s, a)$ to $Q(s', a')$, causing instability as updates to $Q(s, a)$ can negatively impact $Q(s', a')$. To stabilize training, **target networks** are usedâ€”these are periodically updated copies of the main network to break this correlation.  \n",
    "\n",
    "### **Partially Observable MDP (POMDP):**  \n",
    "A **POMDP** is an MDP without the Markov property (future state depends on more than one past state), where the agent has incomplete information about the true state. This occurs in games like poker, where hidden opponent cards create uncertainty. A common solution is **using past observations** to approximate the full state.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from src import dqn_model\n",
    "from src import wrappers\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import typing as tt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 1000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 150000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01\n",
    "\n",
    "State = np.ndarray\n",
    "Action = int\n",
    "BatchTensors = tt.Tuple[\n",
    "    torch.ByteTensor,       # Current State\n",
    "    torch.LongTensor,       # actions\n",
    "    torch.Tensor,           # rewards\n",
    "    torch.BoolTensor,       # done or trunc\n",
    "    torch.ByteTensor,       # next stage\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    state: State\n",
    "    action: Action\n",
    "    reward: float\n",
    "    done_trunc: bool\n",
    "    new_state: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience: Experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
    "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
    "        return [self.buffer[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self.state: tt.Optional[np.ndarray] = None\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state, _ = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net: dqn_model.DQN, device: torch.device,\n",
    "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_v = torch.as_tensor(self.state).to(device)\n",
    "            state_v.unsqueeze(0)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # Do a step in the environment\n",
    "\n",
    "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
    "        self.total_reward += rewarr\n",
    "\n",
    "        exp = Experience(\n",
    "            state = self.state, action = action, reward = float(reward),\n",
    "            done_trunc=is_done or is_tr, new_state=new_state\n",
    "        )\n",
    "\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done or is_tr:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(batch: tt.List[Experience], device: torch.device) ->\n",
    "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
    "    for e in batch:\n",
    "        states.append(e.state)\n",
    "        actions.append(e.action)\n",
    "        rewards.append(e.reward)\n",
    "        dones.append(e.done_trunc)\n",
    "        new_state.append(e.new_state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
