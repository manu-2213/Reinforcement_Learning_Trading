{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf31007d",
   "metadata": {},
   "source": [
    "### Default RL framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3812e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61abc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 100\n",
    "        \n",
    "    def get_observation(self) -> List[float]:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    \n",
    "    def get_actions(self) -> List[int]:\n",
    "        return [0, 1]\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.steps_left == 0\n",
    "    \n",
    "    def action(self, action: int) -> float:\n",
    "        \n",
    "        # Handles and agents action and returns the reward\n",
    "        \n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random() # Reward is random in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711ea981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def step(self, env: Environment):\n",
    "        \"\"\"\n",
    "        Observe the environment\n",
    "        Make a decision about the action to take based on the observations\n",
    "        Submit the action to the environment\n",
    "        Get the reward for the current step\n",
    "        \"\"\"\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3c377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 50.6003\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "    \n",
    "    while not env.is_done():\n",
    "        agent.step(env)\n",
    "        \n",
    "    print(\"Total reward got: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdfcc6f",
   "metadata": {},
   "source": [
    "## **The OpenAI Gym API**\n",
    "\n",
    "At a high level, every environment provides these pieces of information and functionality:\n",
    "- A set of actions that is allowed to be executed in the environment. Gym supports both discrete and continuous actions, as well as their combination\n",
    "- The shape and boundaries of the observations that the environment provides the agent with\n",
    "- A method called step to execute an action, which returns the current observation, the reward, and the indication that the episode is over\n",
    "- A method called reset, which returns the environment to its initial state and obtains the first observation\n",
    "\n",
    "### ***The observation space***\n",
    "\n",
    "The basic abstract class Space includes two methods that are relevant to us:\n",
    "-  sample(): This returns a random sample from the space\n",
    "-  contains(x): This checks whether the argument, x, belongs to the space's domain\n",
    "\n",
    "More classes:\n",
    "- The Discrete class represents a mutually exclusive set of items, numbered from 0 to n – 1. Its only field, n, is a count of the items it describes. For example, Discrete(n=4) can be used for an action space of four directions to move in [left, right, up, or down].\n",
    "- The Box class represents an n-dimensional tensor of rational numbers with intervals [low, high]. For instance, this could be an accelerator pedal with one single value between 0.0 and 1.0, which could be encoded by Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)(the shape argument is assigned a tuple of length 1 with a single value of 1, which gives us a one-dimensional tensor with a single value). The dtype parameter specifies the space's value type and here we specify it as a NumPy 32-bit float. Another example of Box could be an Atari screen observation (we will cover lots of Atari environments later), which is an RGB (red, green, and blue) image of size 210×160: Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8). In this case, the shape argument is a tuple of three elements: the first dimension is the height of the image, the second is the width, and the third equals 3, which all correspond to three color planes for red, green, and blue, respectively. So, in total, every observation is a three-dimensional tensor with 100,800 bytes.\n",
    "- The final child of Space is a Tuple class, which allows us to combine several Space class instances together. This enables us to create action and observation spaces of any complexity that we want. For example, imagine we want to create an action space specification for a car. The car has several controls that can be changed at every timestamp, including the steering wheel angle, brake pedal position, and accelerator pedal position. These three controls can be specified by three float values in one single Box instance. Besides these essential controls, the car has extra discrete controls, like a turn signal (which could be off, right, or left) or horn (on or off). To combine all of this into one action space specification class, we can create Tuple(spaces=(Box(low=-1.0, high=1.0, shape=(3,), dtype=np. float32), Discrete(n=3),Discrete(n=2))). This flexibility is rarely used; for example, in this book, you will see only the Box and Discrete actions and observation spaces, but the Tuple class can be useful in some cases.\n",
    "\n",
    "For example, Discrete.sample() returns a random element from a discrete range, and Box.sample() will be a random tensor with proper dimensions and values lying inside the given range.\n",
    "\n",
    "Every environment has two members of type Space: the action_space and observation_space. \n",
    "\n",
    "### ***The environment***\n",
    "\n",
    "Has the following memebers:\n",
    "\n",
    "- action_space: This is the field of the Space class and provides a specification for allowed actions in the environment.\n",
    "- observation_space: This field has the same Space class, but specifies the observations provided by the environment.\n",
    "- reset(): This resets the environment to its initial state, returning the initial observation vector.\n",
    "- step(): This method allows the agent to take the action and returns information about the outcome of the action – the next observation, the local reward, and the end-of-episode flag.\n",
    "\n",
    "The step() method is the central piece in the environment's functionality. It does\n",
    "several things in one call, which are as follows:\n",
    "- Telling the environment which action we will execute on the next step\n",
    "- Getting the new observation from the environment after this action\n",
    "- Getting the reward the agent gained with this step\n",
    "- Getting the indication that the episode is over\n",
    "\n",
    "The first item (action) is passed as the only argument to this method, and the rest are\n",
    "returned by the step() method. Precisely, this is a tuple (Python tuple and not the\n",
    "Tuple class we discussed in the previous section) of four elements (observation,\n",
    "reward, done, and info). They have these types and meanings:\n",
    "- observation: This is a NumPy vector or a matrix with observation data.\n",
    "- reward: This is the float value of the reward.\n",
    "- done: This is a Boolean indicator, which is True when the episode is over.\n",
    "- info: This could be anything environment-specific with extra information about the environment. The usual practice is to ignore this value in general RL methods (not taking into account the specific details of the particular environment).\n",
    "\n",
    "### ***Creating and environment***\n",
    "\n",
    "To create an environment, the gym package provides the make(env_name) function, whose only argument is the environment's name in string form. The full list of environments can be found at https://gym.openai.com/envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a395b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import warnings\n",
    "# warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f79595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manu13/anaconda3/envs/MLClass/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "e = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc379763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00273233,  0.01887221, -0.02566512, -0.00512351], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The observation of this environment is four floating-point numbers containing\n",
    "# information about the x coordinate of the stick's center of mass, its speed, its angle to\n",
    "# the platform, and its angular speed.\n",
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3295e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The action_space field is of the Discrete type, so our actions will be just 0 or\n",
    "# 1, where 0 means pushing the platform to the left and 1 means to the right.\n",
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c167fa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c59db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7cddb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7718055f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.1253421e-01,  1.9566858e+38,  1.8909226e-01, -2.4372871e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c445a0",
   "metadata": {},
   "source": [
    "### ***random CartPole agent***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c531c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 13 steps, total reward 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manu13/anaconda3/envs/MLClass/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action) # the step() method now returns five values\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" %(total_steps, total_reward))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541db55f",
   "metadata": {},
   "source": [
    "### ***Extra Gym functionality - wrappers and monitors***\n",
    "\n",
    "- Warappers: For situations where you want to \"wrap\" the existing environment and add some extra logic for doing something. Gym provides a convenient framework for these situations – the Wrapper class. The Wrapper class inherits the Env class. Its constructor accepts the only argument – the instance of the Env class to be \"wrapped.\" To add extra functionality, you need to redefine the methods you want to extend, such as step() or reset(). The only requirement is to call the original method of the superclass.  There are subclasses of Wrapper that allow the filtering of only a specific portion of information:\n",
    "\n",
    "    - ObservationWrapper: You need to redefine the observation (obs) method of the parent. The obs argument is an observation from the wrapped environment, and this method should return the observation that will be given to the agent.\n",
    "\n",
    "    - RewardWrapper: This exposes the reward (rew) method, which can modify the reward value given to the agent.\n",
    "    \n",
    "    - ActionWrapper: You need to override the action (act) method, which can tweak the action passed to the wrapped environment by the agent.\n",
    "\n",
    "- Monitors: You can take a look at your agent's life inside the environment. The second argument that we pass to Monitor is the name of the directory that it will write the results to. This directory shouldn't exist, otherwise your program will fail with an exception (to overcome this, you could either remove the existing directory or pass the force=True argument to the Monitor class' constructor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0cd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine a situation where we want to intervene in the stream of actions sent by the agent and, \n",
    "# with a probability of 10%, replace the current action with a random one.\n",
    "\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env: gym.Env, epsilon: float = 0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def action(self, action: gym.core.WrapperActType) -> gym.core.WrapperActType:\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "            print(f\"Random action {action}\")\n",
    "            return action\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b82c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random action 1\n",
      "Random action 0\n",
      "Random action 0\n",
      "Reward got: 11.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make(\"CartPole-v1\"))\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while True:\n",
    "        obs, reward, done, _, _ = env.step(0)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"Reward got: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 15:59:34.177 python[87918:4256515] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-02-19 15:59:34.177 python[87918:4256515] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 24 steps, total reward 24.00\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.HumanRendering(env)\n",
    "    # env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode done in {total_steps} steps, total reward {total_reward:.2f}\")\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
